# # services:
# #   # Service 1: The FastAPI Prediction Service
# #   prediction-service:
# #     build:
# #       context: .
# #       dockerfile: deployment/Dockerfile
# #     container_name: prediction_service_mvp
# #     depends_on:
# #       - mlflow-server
# #     ports:
# #       - "8000:8000"
# #     volumes:
# #       - ./mlruns:/mlruns
# #     environment:
# #       - MLFLOW_TRACKING_URI=http://mlflow-server:5000

# #   # Service 2: The MLflow Tracking Server
# #   mlflow-server:
# #     image: python:3.12-slim
# #     container_name: mlflow_server_mvp
# #     # Use environment variables to configure MLflow. This is the most robust
# #     # method for containerized applications.
# #     environment:
# #       - MLFLOW_BACKEND_STORE_URI=sqlite:////mlruns/mlflow.db
# #       - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlruns/artifacts
# #     # The command now runs gunicorn directly for reliable network binding,
# #     # and it will pick up the configuration from the environment variables above.
# #     command: >
# #       sh -c "pip install --upgrade pip &&
# #              pip install mlflow==3.1.4 sqlalchemy gunicorn &&
# #              gunicorn --bind 0.0.0.0:5000 'mlflow.server:app'"
# #     ports:
# #       - "5000:5000"
# #     volumes:
# #       - ./mlruns:/mlruns

# services:
#   # ─────────────────────────────────────────────────────────────
#   # 1. MLflow Tracking & Model Registry
#   # ─────────────────────────────────────────────────────────────
#   mlflow-server:
#     image: python:3.12-slim
#     container_name: mlflow_server
#     volumes:
#       - ./mlruns:/mlruns
#     ports:
#       - "5000:5000"
#     command: >
#       sh -c "
#         pip install --no-cache-dir --upgrade pip &&
#         pip install --no-cache-dir mlflow==3.1.4 sqlalchemy gunicorn &&
#         gunicorn --bind 0.0.0.0:5000 'mlflow.server:app'
#       "
#     environment:
#       - MLFLOW_BACKEND_STORE_URI=sqlite:////mlruns/mlflow.db
#       - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlruns/artifacts
#       - MLFLOW_ENABLE_ARTIFACTS_ENDPOINT=true
#     healthcheck:
#       test: ["CMD", "curl", "-f", "http://localhost:5000"]
#       interval: 10s
#       timeout: 5s
#       retries: 5

#   # ─────────────────────────────────────────────────────────────
#   # 2. FastAPI Prediction Service  (optional; add when ready)
#   # ─────────────────────────────────────────────────────────────
#   prediction-service:
#     build:
#       context: .
#       dockerfile: deployment/Dockerfile
#     container_name: prediction_service
#     depends_on:
#       mlflow-server:
#         condition: service_healthy
#     volumes:
#       - ./mlruns:/mlruns
#     environment:
#       - MLFLOW_TRACKING_URI=http://mlflow-server:5000
#       - MODEL_NAME=churn-prediction-mvp-lr
#       - MODEL_STAGE=Production
#     ports:
#       - "8000:8000"
